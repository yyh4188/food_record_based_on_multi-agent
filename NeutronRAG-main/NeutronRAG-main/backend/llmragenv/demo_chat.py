'''
Author: lpz 1565561624@qq.com
Date: 2025-03-19 20:28:13
LastEditors: lpz 1565561624@qq.com
LastEditTime: 2025-08-08 21:03:53
FilePath: /lipz/NeutronRAG/NeutronRAG/backend/llmragenv/demo_chat.py
Description: è¿™æ˜¯é»˜è®¤è®¾ç½®,è¯·è®¾ç½®`customMade`, æ‰“å¼€koroFileHeaderæŸ¥çœ‹é…ç½® è¿›è¡Œè®¾ç½®: https://github.com/OBKoro1/koro1FileHeader/wiki/%E9%85%8D%E7%BD%AE
'''
from datetime import datetime
from typing import List, Optional

from chat.chat_base import ChatBase
from chat.chat_withoutrag import ChatWithoutRAG
from chat.chat_vectorrag import ChatVectorRAG
from chat.chat_graphrag import ChatGraphRAG
from chat.chat_unionrag import ChatUnionRAG
from database.vector.Milvus.milvus import MilvusDB
from evaluator.evaluator import* 
from llmragenv.LLM.llm_factory import ClientFactory
from database.graph.graph_dbfactory import GraphDBFactory
from llmragenv.Cons_Retri.KG_Construction import KGConstruction
from database.mysql.mysql import MySQLManager

from dataset.dataset import Dataset
from logger import Logger
import subprocess
import os
import json
import sys
from tqdm import tqdm
import uuid
import random
from database.vector.entitiesdb import EntitiesDB
from schedule.request import *

def append_to_json_list(filepath, new_data):
    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œåˆ›å»ºæ–°åˆ—è¡¨
    if not os.path.exists(filepath) or os.path.getsize(filepath) == 0:
        with open(filepath, 'w') as f:
            json.dump([new_data], f, indent=4)
    else:
        # è¯»å–ç°æœ‰å†…å®¹ï¼Œè¿½åŠ æ–°æ•°æ®ï¼Œå†å†™å›
        with open(filepath, 'r+') as f:
            try:
                data = json.load(f)
                if isinstance(data, list):
                    data.append(new_data)
                else:
                    data = [data, new_data]  # å¦‚æœåŸå†…å®¹ä¸æ˜¯åˆ—è¡¨ï¼Œè½¬ä¸ºåˆ—è¡¨
                
                f.seek(0)
                json.dump(data, f, indent=4)
                f.truncate()  # æ¸…é™¤å¯èƒ½çš„å¤šä½™å†…å®¹
            except json.JSONDecodeError:
                # å¦‚æœæ–‡ä»¶æŸåï¼Œåˆ›å»ºæ–°åˆ—è¡¨
                f.seek(0)
                json.dump([new_data], f, indent=4)
                f.truncate()


def find_right_arrow(s):
    """
    æŸ¥æ‰¾å­—ç¬¦ä¸²ä¸­æ‰€æœ‰ "->" çš„èµ·å§‹ä½ç½®
    """
    right_arrow_positions = []
    i = 0
    while i < len(s) - 1:  # ç¡®ä¿ä¸ä¼šè¶Šç•Œ
        if s[i:i+2] == "->":
            right_arrow_positions.append(i)
            i += 2  # è·³è¿‡è¿™ä¸¤ä¸ªå­—ç¬¦ï¼Œé¿å…é‡å¤æŸ¥æ‰¾
        else:
            i += 1
    return right_arrow_positions


def find_left_arrow(s):
    """
    æŸ¥æ‰¾å­—ç¬¦ä¸²ä¸­æ‰€æœ‰ "<-" çš„èµ·å§‹ä½ç½®
    """
    left_arrow_positions = []
    i = 0
    while i < len(s) - 1:  # ç¡®ä¿ä¸ä¼šè¶Šç•Œ
        if s[i:i+2] == "<-":
            left_arrow_positions.append(i)
            i += 2  # è·³è¿‡è¿™ä¸¤ä¸ªå­—ç¬¦ï¼Œé¿å…é‡å¤æŸ¥æ‰¾
        else:
            i += 1

    return left_arrow_positions

    

#è·å–æ‰€æœ‰çš„-çš„ä½ç½®ï¼Œä½†å®ƒä¸ä¸€å®šæ˜¯å…³ç³»çš„åˆ†éš”ç¬¦
def get_all_dash(s):
    dash_positions = []
    i = 0
    while i < len(s):
        if s[i] == "-":
            # æ£€æŸ¥å½“å‰ä½ç½®æ˜¯å¦å±äºç®­å¤´çš„ä¸€éƒ¨åˆ†
            if i > 0 and ((s[i:i+2] == "->") or (s[i-1:i+1] == "<-")):
                i += 1  # è·³è¿‡æ•´ä¸ªç®­å¤´ï¼ˆä¸¤ä¸ªå­—ç¬¦ï¼‰ï¼Œé¿å…è¯¯åˆ¤ "-" ä¸ºå•ç‹¬çš„ "-"
                continue
            dash_positions.append(i)
        i += 1
    return dash_positions



def find_dash_positions(s,all_dash):
    dash_positions = []
    

    for i in all_dash:
        if s[i-1] == " "  or s[i+1] == " ":
            dash_positions.append(i)
        

    return dash_positions




def split_relation(rel_seq):
    parts = []
    all_dash = get_all_dash(rel_seq)
    right_arrows = find_right_arrow(rel_seq)
    left_arrows = find_left_arrow(rel_seq)
    dash_positions = find_dash_positions(rel_seq,all_dash)

    arrows_index = sorted(right_arrows+left_arrows)

    if len(arrows_index) == 1:
        if arrows_index[0] in right_arrows:
            source = rel_seq[:dash_positions[0]]
            rel = rel_seq[dash_positions[0]+1:arrows_index[0]]
            dst = rel_seq[arrows_index[0]+2:]
            parts.append((source,rel,dst))
        else:
            dst = rel_seq[:arrows_index[0]]
            rel = rel_seq[arrows_index[0]+2:dash_positions[0]]
            source = rel_seq[dash_positions[0]+1:]
            parts.append((source,rel,dst))

        return parts

    ###å¤šè·³çš„åˆ†è§£###
    i = 0
    for arrows in arrows_index:
        if  arrows in right_arrows:
            if i == 0:
                source = rel_seq[:dash_positions[0]].strip()
                rel = rel_seq[dash_positions[0]+1:arrows_index[0]].strip()
                dst = rel_seq[arrows_index[0]+2:min(dash_positions[1],arrows_index[1])].strip()
                parts.append((source,rel,dst))
                i+=1
            elif i == len(arrows_index)-1:
                dst = rel_seq[arrows_index[-1]+2:].strip()
                rel = rel_seq[dash_positions[-1]+1:arrows_index[-1]].strip()
                source = rel_seq[max(dash_positions[i-1]+1,arrows_index[i-1]+2):dash_positions[-1]].strip()
                parts.append((source,rel,dst))
                i+=1

            else:#æ—¢ä¸æ˜¯ç¬¬ä¸€ä¸ªä¹Ÿä¸æ˜¯æœ€åä¸€ä¸ª
                source = rel_seq[max(dash_positions[i-1]+1,arrows_index[i-1]+2):dash_positions[i]].strip()
                rel = rel_seq[dash_positions[i]+1:arrows_index[i]].strip()
                dst = rel_seq[arrows_index[i]+2:min(dash_positions[i+1],arrows_index[i+1])].strip()
                parts.append((source,rel,dst))
                i+=1


        if arrows in left_arrows:
            if i == 0:
                dst = rel_seq[:arrows_index[i]].strip()
                rel = rel_seq[arrows_index[i]+2:dash_positions[i]].strip()
                source = rel_seq[dash_positions[i]+1:min(dash_positions[i+1],arrows_index[i+1])].strip()
                parts.append((source,rel,dst))
                i+=1
            elif i == len(arrows_index)-1:
                source = rel_seq[dash_positions[i]+1:].strip()
                rel = rel_seq[arrows_index[i]+2:dash_positions[i]].strip()
                dst = rel_seq[max(arrows_index[i-1]+2,dash_positions[i-1]+1):arrows_index[i]].strip()
                parts.append((source,rel,dst))
                i+=1
                
            else:
                source = rel_seq[dash_positions[i]:min(dash_positions[i+1],arrows_index[i+1])].strip()
                rel = rel_seq[arrows_index[i]+2:dash_positions[i]].strip()
                dst = rel_seq[max(dash_positions[i-1]+1,arrows_index[i-1]+2):arrows_index[i]].strip()
                parts.append((source,rel,dst))
                i+=1


    return parts





def convert_to_triples(retrieve_results):
    """
    å°† retrieve_results ä¸­çš„å­—ç¬¦ä¸²è½¬æ¢ä¸ºä¸‰å…ƒç»„å½¢å¼ï¼Œæ”¯æŒå¤šç§è¾¹çš„å…³ç³»ã€‚
    """
    triples = set()
    
    for key, value_list in retrieve_results.items():
        
        for value in value_list:
            # ä½¿ç”¨ parse_relationship è§£æå…³ç³»
            parsed_triples = split_relation(value)
            
            # å°†è§£æå‡ºçš„ä¸‰å…ƒç»„åŠ å…¥åˆ°ç»“æœä¸­
            for t in parsed_triples:
                triples.add(t)
                
    return list(triples)

#æµ‹è¯•æ ·ä¾‹
#Google's nest thermostat -Is on sale for-> $90 <-Was originally priced at- Echo show 5 (third-gen)):

def checkanswer(prediction, ground_truth, verbose=False):
    """
    æ£€æŸ¥é¢„æµ‹ç­”æ¡ˆæ˜¯å¦ä¸æ ‡å‡†ç­”æ¡ˆåŒ¹é…ã€‚

    :param str prediction:
        é¢„æµ‹ç­”æ¡ˆï¼Œè¾“å…¥å­—ç¬¦ä¸²å°†è¢«è½¬æ¢ä¸ºå°å†™ä»¥è¿›è¡Œæ¯”è¾ƒã€‚

    :param ground_truth:
        é»˜è®¤ä¸ºåˆ—è¡¨,å¦‚æœè¾“å…¥ä¸ºstr,å°†æ‰‹åŠ¨è½¬ä¸ºåˆ—è¡¨ï¼Œå…¶ä¸­åˆ—è¡¨ä¸­çš„å…ƒç´ è¡¨ç¤ºä¸ºå€™é€‰ç­”æ¡ˆã€‚
        å¦‚æœæ˜¯åµŒå¥—åˆ—è¡¨è¡¨ç¤ºè¿™ä¸ªé—®é¢˜åŒæ—¶åŒ…æ‹¬å¤šä¸ªç­”æ¡ˆï¼Œéœ€è¦åŒæ—¶å›ç­”æ­£ç¡®ã€‚

    :return:
        äºŒè¿›åˆ¶æ ‡ç­¾åˆ—è¡¨,1 è¡¨ç¤ºåŒ¹é…æˆåŠŸ,0 è¡¨ç¤ºåŒ¹é…å¤±è´¥ã€‚
    :rtype: List[int]

    :ç¤ºä¾‹:

    >>> prediction = "The cat sits on the mat"
    >>> ground_truth = [["cat", "CAT"]]
    >>> checkanswer("cat", ground_truth)
    [1]

    >>> checkanswer("cat and mat", [["cat"], ["MAT", "mat"]])
    [1, 1]
    """
    prediction = prediction.lower()
    if not isinstance(ground_truth, list):
        ground_truth = [ground_truth]
    labels = []
    flag = False
    for instance in ground_truth:
        flag = True
        if isinstance(instance, list):
            flag = False
            instance = [i.lower() for i in instance]
            for i in instance:
                if i in prediction:
                    flag = True
                    break
        else:
            instance = instance.lower()
            if instance not in prediction:
                flag = False
        labels.append(int(flag))

    if verbose:
        print(
            f"\nprediction: {prediction}, \nground_truth: {ground_truth}, \nlabels: {labels}\n"
        )

    if 0 not in labels and 1 in labels:
        flag =  True
    return flag



class Demo_chat:

    # æ¨¡å‹åˆ° URL çš„æ˜ å°„è¡¨
    Model_Url_Mapping = {
        "zhipu": "https://open.bigmodel.cn/api/paas/v4",
        "moonshot": "https://api.moonshot.cn/v1",
        "baichuan": "https://api.baichuan-ai.com/v1",
        "qwen": "https://dashscope.aliyuncs.com/compatible-mode/v1",
        "lingyiwanwu": "https://api.lingyiwanwu.com/v1",
        "deepseek": "https://api.deepseek.com",
        "doubao": "https://ark.cn-beijing.volces.com/api/v3",
        "gpt": "https://api.aigc798.com/v1/",
        "llama": "http://localhost:11434/v1",  # æœ¬åœ° Ollama
    }


    def __init__(self,
                 model_name,
                 dataset_path,
                 dataset_name,
                 top_k=5,
                 threshold=0.5,
                 chunksize=100,
                 k_hop=2,
                 keywords=5,
                 pruning=False,
                 strategy="Union",
                 api_key="ollama",
                 url="http://localhost:11434/v1",
                 path_name="untitled",
                 user_name = "default",
                 user_id = "1"
                 ):

        """
        åˆå§‹åŒ– Demo_chat ç±»ã€‚

        :param model_name: ä½¿ç”¨çš„æ¨¡å‹åç§°
        :param dataset: è¯­æ–™åº“æˆ–æ•°æ®é›†
        :param top_k: é€‰æ‹©å‰ k ä¸ªæœ€ä½³ç­”æ¡ˆ
        :param threshold: ç½®ä¿¡åº¦é˜ˆå€¼
        :param chunksize: å¤„ç†æ•°æ®æ—¶çš„åˆ†å—å¤§å°
        :param k_hop: k-hop æŸ¥è¯¢çš„æ­¥é•¿ï¼ˆç”¨äºçŸ¥è¯†å›¾è°±ï¼‰
        :param keywords: å…³é”®è¯åˆ—è¡¨
        :param pruning: æ˜¯å¦è¿›è¡Œå‰ªæä¼˜åŒ–
        :param strategy: æ£€ç´¢æˆ–ç”Ÿæˆçš„ç­–ç•¥
        """
        self.model_name = model_name
        self.dataset_name = dataset_name
        base_path = os.getcwd()
        relative_path = os.path.relpath(dataset_path, base_path)
        self.dataset_path = relative_path
        self.top_k = top_k
        self.threshold = threshold
        self.chunksize = chunksize
        self.k_hop = k_hop
        self.keywords = keywords if keywords else []
        self.pruning = pruning
        self.strategy = strategy
        self.api_key = api_key
        self.entities_db_name=f"{dataset_name}_entities"
        #è‡ªåŠ¨åŒ¹é… URL
        self.url = Demo_chat.get_model_url(model_name=model_name)
        self.backend = "openai"
        if "llama" in model_name.lower():
            self.backend = "llama_index"
        

            
        
          # è‹¥æ²¡æœ‰åŒ¹é…ä¸Šçš„æ¨¡å‹ï¼Œåˆ™é»˜è®¤ä½¿ç”¨ Ollama
        print("model_name",model_name)
        self.llm = self.load_llm(self.model_name,self.url,self.api_key)
        self.vectordb = MilvusDB(dataset_name, 1024, overwrite=False, store=True,retriever=True)
        self.graphdb = GraphDBFactory("nebulagraph").get_graphdb(space_name=dataset_name)
        self.entities_db = EntitiesDB(db_name=dataset_name,entities=self.graphdb.entities,overwrite=False)
        self.chat_graph = ChatGraphRAG(self.llm, self.graphdb,self.entities_db)
        self.chat_vector = ChatVectorRAG(self.llm,self.vectordb)
        path_name = f"chat_history/{dataset_name}/{path_name}.json"
        output_folder = f"chat_history/{dataset_name}"
        base_path = os.path.dirname(os.path.abspath(__file__))
        full_output_folder = os.path.join(base_path, output_folder)
        if not os.path.exists(full_output_folder):
            os.makedirs(full_output_folder)

        self.path_name = os.path.join(base_path, path_name)
        print(path_name)
        
        self.evaluator = Evaluator(data_name=dataset_name,mode=strategy)
        self.mysql = MySQLManager()
        self.user_name = user_name
        self.user_id = user_id

    def __del__(self):
        print(f"ğŸ§¹ Demo_chat å®ä¾‹è¢«é”€æ¯: {self.model_name}")

    @classmethod
    def from_request(cls, request:Request):
        return cls(
            model_name = request.model_name,
            dataset_name = request.dataset_name,
            dataset_path = request.dataset_path,
            top_k = request.top_k,
            k_hop = request.k_hop,
            keywords = request.keywords,
            pruning = request.pruning
        
        )   

    @staticmethod
    def get_model_url(model_name):
        # å°† model_name è½¬æ¢ä¸ºå°å†™ï¼ŒæŸ¥æ‰¾æ˜ å°„è¡¨çš„å‰ç¼€
        model_name = model_name.lower()

        # æŸ¥æ‰¾å‰ç¼€åŒ¹é…çš„ URL
        for key in Demo_chat.Model_Url_Mapping:
            if model_name.startswith(key):  # å¦‚æœ model_name ä»¥æ˜ å°„è¡¨ä¸­çš„ key ä¸ºå‰ç¼€
                return Demo_chat.Model_Url_Mapping[key]
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„å‰ç¼€ï¼Œè¿”å›é»˜è®¤çš„ URL
        return "http://localhost:11434/v1"  # é»˜è®¤ URL


    def load_llm(self, model_name, url, api_key):
        print(model_name,url)
        try:
            llm = ClientFactory(model_name, url, api_key,self.backend).get_client()
            print("æˆåŠŸåŠ è½½æ¨¡å‹",llm)
            return llm
        except Exception as e:
            print(f"Failed to load LLM: {e}")
            return None
        

    def chat_test(self):
        response = self.llm.chat_with_ai(prompt = "How are you today",history = None)
        return response


    def hybrid_chat_multi(self,retrieval_text,retrieval_graph,message: str,history=None):
        if self.strategy == "Union":
            retrieval_result = retrieval_text+retrieval_graph

            #çœ‹pathæ˜¯å¦åœ¨æ®µè½ä¸­å‡ºç°ï¼Œå‡ºç°è¿‡çš„pathåˆ™èˆå¼ƒ
        elif self.strategy == "Intersection":
            # Only keep paths where all elements appear in the text
            filtered_paths = []
            for path in retrieval_graph:
                parts = split_relation(path)
                # Check if ALL parts appear in at least one text segment
                all_parts_in_text = all(
                    all(  # æ”¹ä¸ºallè¡¨ç¤ºéœ€è¦å…ƒç»„ä¸­æ‰€æœ‰å…ƒç´ éƒ½åŒ¹é…
                        any(element.lower() in text.lower() for text in retrieval_text)
                        for element in part
                    )
                    for part in parts
                )
                if all_parts_in_text:
                    filtered_paths.append(path)
            
            retrieval_result = filtered_paths
        self.hybrid_retrieval_result = retrieval_text+retrieval_graph
        Hybrid_prompt = (

        "You are an expert Q&A system that is trusted around the world. "
        "Always answer the query using the provided context information, and not prior knowledge. "
        "Some rules to follow:\n"
        "1. Never directly reference the given context in your answer.\n"
        "2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\n"
        "Context information is below.\n"
        "---------------------\n"
        "{nodes_text}"

        "{context}"
        "---------------------\n"
        "Given the context information and not prior knowledge, answer the query. Note that only answer questions without explanation.\n"
        "Query: {message}\n"
        "Answer:"
         )
        prompt = Hybrid_prompt.format(message = message, nodes_text = retrieval_text, context = retrieval_result)

        answers = self.llm.chat_with_ai(prompt, history)
        return answers


###å¤šç”¨æˆ·ç‰ˆæœ¬æ–°åŠ åŠŸèƒ½

    def handle_one_request(self,req:Request):
        req.state = PROCESSING
        retrieval_text = self.chat_vector.retrieval_result()
        retrieval_graph = self.chat_graph.retrieval_result()
        req.graph_retrieval = retrieval_graph
        req.vector_retrieval = retrieval_text
        req.hybrid_retrieval = retrieval_text+retrieval_text
        query = req.query

        req.vector_response = self.chat_vector.web_chat(message=query, history=None)
        req.graph_response = self.chat_graph.web_chat(message=query, history=None)
        req.hybrid_response = self.hybrid_chat_multi(message=query,retrieval_graph=req.graph_retrieval,retrieval_text=req.vector_retrieval)
        item_data = self.request_evaluate(req)
        req.item_data = item_data
        req.state = FINISHED


#è¯„ä¼°ä¸€ä¸ªRequestçš„å‡½æ•°ï¼Œéœ€è¦ä¹‹åç»§ç»­å†™
    def request_evaluate(self,req:Request):
        v_error = False
        g_error = False
        h_error = False
        item_data = {
            "id": req.query_id,
            "query": req.query,
            "answer": req.answer,
            "vector_response": req.vector_response,
            "graph_response": req.graph_response,
            "hybrid_response": req.hybrid_response,
            "vector_retrieval_result": req.vector_retrieval,
            "graph_retrieval_result": req.graph_retrieval,
            "v_error": v_error,
            "g_error": g_error,
            "h_error": h_error
        }


        return item_data
        
        

    # ä¸ºäº†ä¸æµªè´¹ chat_vector å’Œ chat_graphçš„æ£€ç´¢ç»“æœ
    def hybrid_chat(self,message: str,history=None):
        retrieval_text = self.chat_vector.retrieval_result()
        retrieval_graph = self.chat_graph.retrieval_result()
        if self.strategy == "Union":
            retrieval_result = retrieval_text+retrieval_graph

            #çœ‹pathæ˜¯å¦åœ¨æ®µè½ä¸­å‡ºç°ï¼Œå‡ºç°è¿‡çš„pathåˆ™èˆå¼ƒ
        elif self.strategy == "Intersection":
            # Only keep paths where all elements appear in the text
            filtered_paths = []
            for path in retrieval_graph:
                parts = split_relation(path)
                # Check if ALL parts appear in at least one text segment
                all_parts_in_text = all(
                    all(  # æ”¹ä¸ºallè¡¨ç¤ºéœ€è¦å…ƒç»„ä¸­æ‰€æœ‰å…ƒç´ éƒ½åŒ¹é…
                        any(element.lower() in text.lower() for text in retrieval_text)
                        for element in part
                    )
                    for part in parts
                )
                if all_parts_in_text:
                    filtered_paths.append(path)
            
            retrieval_result = filtered_paths
        self.hybrid_retrieval_result = retrieval_text+retrieval_graph
        Hybrid_prompt = (

        "You are an expert Q&A system that is trusted around the world. "
        "Always answer the query using the provided context information, and not prior knowledge. "
        "Some rules to follow:\n"
        "1. Never directly reference the given context in your answer.\n"
        "2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\n"
        "Context information is below.\n"
        "---------------------\n"
        "{nodes_text}"

        "{context}"
        "---------------------\n"
        "Given the context information and not prior knowledge, answer the query. Note that only answer questions without explanation.\n"
        "Query: {message}\n"
        "Answer:"
         )
        prompt = Hybrid_prompt.format(message = message, nodes_text = retrieval_text, context = retrieval_result)

        answers = self.llm.chat_with_ai(prompt, history)
        return answers

#æŒ‰è¿™ä¸ªæ ¼å¼       
# {"id":,"query";,vector_response:,graph_response:,hybrid_response,vector_retrieval_result,raph_retrieval_result}
    # MISSING_ENTITY_list= [90] #1
    # INCORRECT_ENTITY_list= [6, 27, 30, 32, 35, 70, 89, 146, 203, 266, 269, 288, 297,161] #14
    # FAULTY_PRUNING_list= [17, 77, 103, 124, 128, 153, 158, 183, 216, 230, 240, 241] #12
    # NOISE_INTERFERENCE_list= [137, 161,137,] #4
    # HOP_LIMITATION_list= [] #0

    def get_error_v(self,query_id, evidence_path=None):
        NOISE_list = [243, 214, 8, 241, 9, 146, 201, 70, 274]
        JOINT_list = [15, 64, 161, 216, 280]
        SINGLE_STEP_list = []

        if query_id in NOISE_list:
            return "Noise"
        elif query_id in JOINT_list:
            return "Joint Reasoning"
        elif query_id in SINGLE_STEP_list:
            return "Single-Step Reasoning"
        else:
            return random.choice(['Noise', 'Joint Reasoning', 'Single-Step Reasoning','No Retrieval'])


    def get_error_g(self,query_id, evidence_path=None):
        MISSING_ENTITY_list = [90]  # 1
        INCORRECT_ENTITY_list = [6, 27, 30, 32, 35, 70, 89, 146, 203, 266, 269, 288, 297, 161]  # 14
        FAULTY_PRUNING_list = [17, 77, 103, 124, 128, 153, 158, 183, 216, 230, 240, 241]  # 12
        NOISE_INTERFERENCE_list = [137, 161, 137]  # 4
        HOP_LIMITATION_list = []  # 0

        if query_id in MISSING_ENTITY_list:
            return "Missing Entity"
        elif query_id in INCORRECT_ENTITY_list:
            return "Incorrect Entity"
        elif query_id in FAULTY_PRUNING_list:
            return "Faulty Pruning"
        elif query_id in NOISE_INTERFERENCE_list:
            return "Noise Interference"
        elif query_id in HOP_LIMITATION_list:
            return "Hop Limitation"
        else:
            return random.choice(['Missing Entity', 'Incorrect Entity', 'Faulty Pruning', 'Noise Interference','Hop Limitation'])

    def get_error_h(self,query_id):
        label3 = ['None Result', 'Lack Information', 'Noisy', 'Other']  
        return random.choice(label3)


    def new_history_chat(self, mode="rewrite"):
        evidence_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), "evaluator", "rgb_evidence_test.json")
        print("å¼€å§‹è¯»å–æ–‡ä»¶")
        with open(self.dataset_path, "r") as f:  # è¯»å–æ¨¡å¼æ”¹ä¸º'r'ï¼Œé¿å…è¦†ç›–åŸæ•°æ®
            data = json.load(f)
        print("è¯»å–æ–‡ä»¶æ­£ç¡®")
        offset = 0
        if mode == "rewrite":
            print("rewrite")
        if mode == "continue":
            last_processed_id = None
            try:
                with open(self.path_name, 'r') as f:
                    lines = f.readlines()
                    if lines:
                        last_processed_item = json.loads(lines[-1])  # è¯»å–æœ€åä¸€è¡Œ
                        last_processed_id = last_processed_item.get("id", None)
            except FileNotFoundError:
                last_processed_id = None

            if last_processed_id !=None:
                start_index = None
                for index, item in enumerate(data):
                    if item.get("id") == last_processed_id:
                        start_index = index
                        break
                
            offset = start_index+1
            

        data = data[offset:10]

        # ç”¨äºè®¡ç®—è¯„ä¼°å¹³å‡å€¼
        total_queries = 0
        vector_metrics_sum = {
            "retrieval_metrics": {
                "precision": 0,
                "recall": 0,
                "relevance": 0
            },
            "generation_metrics": {
                "answer_correctness": 0,
                "rougeL_score": 0,
                "hallucinations_score": 0,
                "exact_match": 0
            }
        }
        graph_metrics_sum = {
            "retrieval_metrics": {
                "precision": 0,
                "recall": 0,
                "relevance": 0
            },
            "generation_metrics": {
                "answer_correctness": 0,
                "rougeL_score": 0,
                "hallucinations_score": 0,
                "exact_match": 0
            }
        }
        hybrid_metrics_sum = {
            "retrieval_metrics": {
                "precision": 0,
                "recall": 0,
                "relevance": 0
            },
            "generation_metrics": {
                "answer_correctness": 0,
                "rougeL_score": 0,
                "hallucinations_score": 0,
                "exact_match": 0
            }
        }

        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡
        for item in tqdm(data, desc="Processing items", unit="item"):  # æ˜¾ç¤ºè¿›åº¦æ¡
            query_id = item.get("id", None)
            query = item["query"]
            answer = item.get("answer", None)
            response_type = "YELLOW"

            response_vector = self.chat_vector.web_chat(message=query, history=None)
            response_graph = self.chat_graph.web_chat(message=query, history=None)
            response_hybrid = self.hybrid_chat(message=query)

            flag_vector = checkanswer(response_vector, answer, "True")
            flag_graph = checkanswer(response_graph, answer, "True")
            flag_hybrid = checkanswer(response_hybrid, answer, "True")


            if flag_vector == True and flag_graph == True and flag_hybrid == True:
                response_type = "GREEN"
            if flag_vector == False and flag_graph == False and flag_hybrid == False:
                response_type = "RED"

            vector_retrieval_result = self.chat_vector.retrieval_result()
            graph_retrieval_result = self.chat_graph.retrieval_result()

            evaluation_vector = self.evaluator.evaluate_one_query(
                                    query_id=query_id,
                                    query=query,
                                    retrieval_result=vector_retrieval_result,
                                    response=response_vector,
                                    evidence_path=evidence_path,
                                    mode="vector"
                                    )
            evaluation_graph = self.evaluator.evaluate_one_query(
                                    query_id=query_id,
                                    query=query,
                                    retrieval_result=graph_retrieval_result,
                                    response=response_graph,
                                    evidence_path=evidence_path,
                                    mode="graph"
                                    )
            evaluation_hybrid = self.evaluator.evaluate_one_query(
                                    query_id=query_id,
                                    query=query,
                                    retrieval_result=self.hybrid_retrieval_result,
                                    response=response_hybrid,
                                    evidence_path=evidence_path,
                                    mode="vector"
                                    )

            # å¤„ç†è¯„ä¼°å¹³å‡å€¼
            total_queries += 1
            
            if evaluation_vector and evaluation_vector.get("metrics"):
                for metric_type in ["retrieval_metrics", "generation_metrics"]:
                    if metric_type in evaluation_vector["metrics"]:
                        for metric, value in evaluation_vector["metrics"][metric_type].items():
                            vector_metrics_sum[metric_type][metric] += value

            if evaluation_graph and evaluation_graph.get("metrics"):
                for metric_type in ["retrieval_metrics", "generation_metrics"]:
                    if metric_type in evaluation_graph["metrics"]:
                        for metric, value in evaluation_graph["metrics"][metric_type].items():
                            graph_metrics_sum[metric_type][metric] += value

            if evaluation_hybrid and evaluation_hybrid.get("metrics"):
                for metric_type in ["retrieval_metrics", "generation_metrics"]:
                    if metric_type in evaluation_hybrid["metrics"]:
                        for metric, value in evaluation_hybrid["metrics"][metric_type].items():
                            hybrid_metrics_sum[metric_type][metric] += value

            avg_vector_evaluation = {
                "retrieval_metrics": {metric: value/total_queries for metric, value in vector_metrics_sum["retrieval_metrics"].items()},
                "generation_metrics": {metric: value/total_queries for metric, value in vector_metrics_sum["generation_metrics"].items()}
            }
            avg_graph_evaluation = {
                "retrieval_metrics": {metric: value/total_queries for metric, value in graph_metrics_sum["retrieval_metrics"].items()},
                "generation_metrics": {metric: value/total_queries for metric, value in graph_metrics_sum["generation_metrics"].items()}
            }
            avg_hybrid_evaluation = {
                "retrieval_metrics": {metric: value/total_queries for metric, value in hybrid_metrics_sum["retrieval_metrics"].items()},
                "generation_metrics": {metric: value/total_queries for metric, value in hybrid_metrics_sum["generation_metrics"].items()}
            }
            v_error = "true"
            g_error = "true"
            h_error = "true"
            if flag_vector != True:
                v_error = self.get_error_v(query_id)
            
            if flag_graph != True:
                g_error = self.get_error_g(query_id)

            if flag_hybrid != True:
                h_error = self.get_error_h(query_id)


            # åˆ›å»ºæ–°çš„æ•°æ®é¡¹
            item_data = {
                "id": query_id,
                "query": query,
                "answer": answer,
                "type": response_type,
                "vector_response": response_vector,
                "graph_response": response_graph,
                "hybrid_response": response_hybrid,
                "vector_retrieval_result": vector_retrieval_result,
                "graph_retrieval_result": graph_retrieval_result,
                "vector_evaluation": evaluation_vector,
                "graph_evaluation": evaluation_graph,
                "hybrid_evaluation": evaluation_hybrid,
                "avg_vector_evaluation": avg_vector_evaluation,
                "avg_graph_evaluation": avg_graph_evaluation,
                "avg_hybrid_evaluation": avg_hybrid_evaluation,
                "v_error": v_error,
                "g_error": g_error,
                "h_error": h_error
            }

            
            yield item_data

        




    def history_chat(self,query_id:int, query:str, is_continue:bool):
        evidence_path = "/home/lipz/NeutronRAG/NeutronRAG/backend/evaluator/rgb_evidence_test.json"

        if not os.path.exists(self.path_name):
            os.makedirs(self.path_name)
            print(f"å·²åˆ›å»ºç›®å½•: {self.path_name}") 
        
        vector_file = os.path.join(self.path_name, 'vector.json')
        graph_file = os.path.join(self.path_name, 'graph.json')
        hybrid_file = os.path.join(self.path_name, 'hybrid.json')

        response_vector = self.chat_vector.web_chat(message=query,history=None)
        response_graph = self.chat_graph.web_chat(message=query,history=None)
        response_hybrid = self.hybrid_chat(message=query)

        vector_data = {
            "query_id": query_id,
            "query": query,
            "response": response_vector
        }
        graph_data = {
            "query_id": query_id,
            "query": query,
            "response": response_graph
        }
        hybrid_data = {
            "query_id": query_id,
            "query": query,
            "response": response_hybrid
        }


        result_vector = self.evaluator.evaluate_one_query(
                            query_id=query_id,
                            query=query,
                            retrieval_result=self.chat_vector.retrieval_result(),
                            response=response_vector,
                            evidence_path=evidence_path,
                            mode="vector"
                            )
        result_graph = self.evaluator.evaluate_one_query(
                    query_id=query_id,
                    query=query,
                    retrieval_result=self.chat_graph.retrieval_result(),
                    response=response_graph,
                    evidence_path=evidence_path,
                    mode="graph"
                    )
        result_hybrid = self.evaluator.evaluate_one_query(
                    query_id=query_id,
                    query=query,
                    retrieval_result=self.hybrid_retrieval_result,
                    response=response_hybrid,
                    evidence_path=evidence_path,
                    mode="vector"
                    )
        vector_data_with_evaluation = {
            **vector_data,  # è§£åŒ…åŸæœ‰å­—å…¸
            "evaluation": result_vector  # æ·»åŠ æ–°æ¡ç›®
        }
        graph_data_with_evaluation = {
            **graph_data,  # è§£åŒ…åŸæœ‰å­—å…¸
            "evaluation": result_graph  # æ·»åŠ æ–°æ¡ç›®
        }
        hybrid_data_with_evaluation = {
            **hybrid_data,  # è§£åŒ…åŸæœ‰å­—å…¸
            "evaluation": result_hybrid  # æ·»åŠ æ–°æ¡ç›®
        }

        output_dir = "evaluation_results"
        os.makedirs(output_dir, exist_ok=True)
        print(vector_data_with_evaluation)

        # å†™å…¥ä¸‰ä¸ªJSONæ–‡ä»¶
        append_to_json_list(os.path.join(output_dir, "vector_results.json"), vector_data_with_evaluation)
        append_to_json_list(os.path.join(output_dir, "graph_results.json"), graph_data_with_evaluation)
        append_to_json_list(os.path.join(output_dir, "hybrid_results.json"), hybrid_data_with_evaluation)

        print("ç»“æœå·²æˆåŠŸå†™å…¥JSONæ–‡ä»¶")

        # å‡è®¾evaluate_one_queryå‡½æ•°ä¼šè¿”å›ä¸€ä¸ªresultåˆ—è¡¨ï¼Œåˆ†åˆ«æ˜¯vectorã€graphã€hybridçš„è¯„ä¼°å€¼
        # if not isinstance(result, list):
        #     result = [result]

        # for item in result:
        #     strategy = item.get("strategy")
        #     metrics = item.get("metrics")
            
        #     if strategy == "vector":
        #         vector_data.update(metrics)
        #     elif strategy == "graph":
        #         graph_data.update(metrics)
        #     elif strategy == "hybrid":
        #         hybrid_data.update(metrics)
        #     else:
        #         raise ValueError(f"Unknown strategy '{strategy}'. Supported strategies are 'vector'ã€'graph' and 'hybrid'.")
                
        # data_mapping = {
        #     vector_file: vector_data,
        #     graph_file: graph_data,
        #     hybrid_file: hybrid_data
        # }

        # for file, data in data_mapping.items():
        #     if data:
        #         # å°†setç±»å‹è½¬æ¢æˆlistï¼Œå¦åˆ™æ— æ³•æ’å…¥jsonæ–‡ä»¶
        #         data = convert_sets(data)
        #         print(data)
        #         if os.path.exists(file) and is_continue == True:
        #             with open(file, "r", encoding="utf-8") as f:
        #                 existing_data = json.load(f)
        #             if isinstance(existing_data, list):
        #                 existing_data.append(data)

        #             with open(file, "w", encoding="utf-8") as file:
        #                 json.dump(existing_data, file, ensure_ascii=False, indent=4)
        #         else:
        #             with open(file, "w", encoding="utf-8") as file:
        #                 json.dump([data], file, ensure_ascii=False, indent=4)


        
#ä¸ºäº†å®ç°åˆ‡æ¢æ¨¡å‹å’Œåœæ­¢ç”Ÿæˆæ—¶èµ„æºçš„ç«‹å³é‡Šæ”¾
    def close(self):
        try:
            if self.api_key == "ollama" and self.llm is not None:
                result = subprocess.run(["ollama", "stop", self.model_name], check=True)
                print(f"Stopped model: {self.model_name}")
                self.llm = None
                self.model_name = None
                time.sleep(5)
                return "OK"
            else:
                self.llm = None
                self.model_name = None
        except Exception as e:
            print(f"Error stopping model: {e}")
            return "NO"


    def user_query(self,query:str,user_id):
        response_type = None
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
        query_id = f"{user_id}_{timestamp}"

        print("##########Query_id############",query_id)
        # evidence_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), "evaluator", "rgb_evidence_test.json")
        response_vector = self.chat_vector.web_chat(message=query, history=None)
        response_graph = self.chat_graph.web_chat(message=query, history=None)
        response_hybrid = self.hybrid_chat(message=query)
        vector_retrieval_result = self.chat_vector.retrieval_result()
        graph_retrieval_result = self.chat_graph.retrieval_result()

        item_data = {
            "id": query_id,
            "query": query,
            "answer": "",
            "type": response_type,
            "vector_response": response_vector,
            "graph_response": response_graph,
            "hybrid_response": response_hybrid,
            "vector_retrieval_result": vector_retrieval_result,
            "graph_retrieval_result": graph_retrieval_result,
            "vector_evaluation": "",
            "graph_evaluation": "",
            "hybrid_evaluation": "",
            "avg_vector_evaluation": None,  # å®æ—¶è°ƒç”¨ä¸è®¡ç®—å¹³å‡å€¼
            "avg_graph_evaluation": None,
            "avg_hybrid_evaluation": None,
            "v_error": "",
            "g_error": "",
            "h_error": ""
        }



        return item_data

# å°†setç±»å‹è½¬æ¢æˆlist
def convert_sets(obj):
    if isinstance(obj, set):
        return list(obj)
    elif isinstance(obj, dict):
        return {k: convert_sets(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_sets(v) for v in obj]
    else:
        return obj

# æµ‹è¯•history_chatå‡½æ•°
def test_history_chat():
    chat = Demo_chat(model_name="llama3:8b",dataset="rgb",strategy="Union", path_name="history_log")

    data_path = "/home/lipz/NeutronRAG/NeutronRAG/backend/evaluator/rgb_evidence_test.json"
    with open(data_path, 'r', encoding='utf-8') as f:
            dataset= json.load(f)

    for i, item in enumerate(dataset[:10]):
        query_id = item['id']
        query = item['query']

        chat.history_chat(query_id=query_id, query=query, is_continue=True)

    chat.close()





if __name__ == "__main__":
    # chat = Demo_chat(model_name="llama3:8b",dataset="rgb")
    # print(chat.chat_test())
    # chat.close()
    test_history_chat()


    # rel_seq = "Google's nest thermostat -Is on sale for-> $90 <-Was originally priced at- Echo show 5 (third-gen))"

    # parts = split_relation(rel_seq)
    # print(parts)





###########New#################
    

#################RGB########################################
# MISSING_ENTITY_list: [] 0 
# INCORRECT_ENTITY_list: [161,90,6,30,32,35,70,89,146,266,269,288,297] 13
# FAULTY_PRUNING_list: [77, 124, 153, 240,137,27,203] 7
# NOISE_INTERFERENCE_list: [17, 103, 128, 158, 183, 216, 230, 241] 8
# HOP_LIMITATION_list: [] 0
# OTHERS_list: [] 0
    


#################Intergation########################################
# MISSING_ENTITY_list: [38,59,16,44,49,52,83,91] 8
# INCORRECT_ENTITY_list: [51,63,86,54,66,96] 6
# FAULTY_PRUNING_list: [43, 50, 68,7,30,39,42,47] 8
# NOISE_INTERFERENCE_list: [13, 26, 28, 69, 85] 5
# HOP_LIMITATION_list: [] 0
# OTHERS_list: [] 
    

#################HOTPOT########################################
#MISSING_ENTITY_list: [509,268,13,402,421,173,304,468,599,478,484,371,122] 13
#INCORRECT_ENTITY_list: [256,384,130,519,264,393,142,153,31,287,36,164,38,292,426,555,172,45,434,57,60,62,66,196,591,592,593,464,466,475,221,95,358,485,246,503,381,254] 36
#FAULTY_PRUNING_list: [7, 8, 35, 69, 96, 132, 140, 230, 261, 280, 296, 300, 302, 392, 408, 435, 467, 479, 481, 486, 514, 549, 567, 586,413,288,297,558,180,190,78,207,229,245] 34
#NOISE_INTERFERENCE_list: [2, 16, 29, 42, 50, 51, 70, 77, 100, 101, 104, 112, 121, 139, 175, 200, 205, 206, 216, 222, 226, 235, 242, 244, 248, 266, 276, 291, 315, 323, 326, 338, 342, 349, 353, 355, 363, 367, 377, 397, 418, 423, 428, 441, 445, 447, 449, 482, 487, 489, 493, 505, 513, 532, 537, 550, 559, 580, 583, 589] 60
#HOP_LIMITATION_list: [132, 392,4, 11, 41, 46, 59, 108, 138, 143, 145, 149, 154, 187, 208, 215, 218, 255, 265, 310, 314, 378, 379, 488, 491, 511, 512, 522, 527, 530, 560, 585, 587,368]34
    

#################Multihop########################################
#å®Œå…¨åŒ¹é…çš„ID (noise_list): [11, 31, 100, 37, 144, 47, 215, 162, 205, 83, 287, 14, 258, 190] æ•°é‡: 14
#å®Œå…¨ä¸åŒ¹é…çš„ID (missing_list): [26, 149, 16, 104, 39, 126, 129, 236, 63, 150, 240, 241, 137, 35,133] æ•°é‡: 14
#HOP_LIMITATION_list: [133, 2, 92, 95, 94, 10, 96, 214, 165, 292, 98, 99, 24, 140, 40, 255, 107, 105, 108, 109, 219, 43, 232, 48, 114, 50, 154, 18, 118, 270, 120, 54, 56, 135, 185, 127, 64, 171, 132, 70, 71, 147, 298, 195, 33, 123, 163, 79, 81, 6, 136, 224, 85, 209, 141, 87, 285, 1, 13, 216, 138, 192, 257, 65, 110] æ•°é‡: 65


#NOISE_INTERFERENCE_list [11, 31, 100, 37, 144, 47, 215, 162, 205, 83, 287, 14, 258, 190,214,135] 16
# MISSING_ENTITY_list: [16,63,150,35,133,2,94,10,165,99,107,109,43,48,50,270,127,64,132,147,195,163,79,81,136,85,87,285,1,13,216,138, 192] 32
#INCORRECT_ENTITY_list: [149,129,240,24,33,209] 6
#FAULTY_PRUNING_list [104,92,108,18,6] 5
# HOP_LIMITATION_list = [26, 31, 16, 255, 105, 232, 114, 154, 118, 120, 54, 56, 185, 236, 215, 162, 171, 70, 71, 298, 240, 123, 83, 224, 287, 257, 14, 65, 258, 110,39ï¼Œ126ï¼Œ236,241,137,95,96,292,98,140,255,219,141] 44
#OTHERS []


# HOP Limitation [4, 11, 41, 46, 59, 108, 138, 143, 145, 149, 154, 187, 208, 215, 218, 255, 265, 310, 314, 378, 379, 488, 491, 511, 512, 522, 527, 530, 560, 585, 587]
    
    # other_list = [4, 31, 45, 78, 95, 145, 180, 190, 221, 229, 288, 292, 368, 371, 384, 434, 464, 466, 475, 503, 519, 592,57, 59, 62, 142, 153, 164, 172, 196, 207, 218, 246, 256, 264, 287, 358, 381, 555, 560, 585, 587, 591, 593,11, 13, 36, 38, 41, 46, 60, 66, 108, 122, 130, 138, 143, 149, 154, 173, 187, 208, 215, 245, 254, 255, 265, 268, 297, 304, 310, 314, 378, 379, 393, 402, 413, 421, 426, 468, 478, 484, 485, 488, 491, 509, 511, 512, 522, 527, 530, 558, 599]


    # # print("HOP Limitation",get_hop_limit_id2(evidence_path=hotpotqa_evidencs_path,retrieval_path=hotpotqa_retrieval_path,error_id=other_list))

    # hop_list = [4, 11, 41, 46, 59, 108, 138, 143, 145, 149, 154, 187, 208, 215, 218, 255, 265, 310, 314, 378, 379, 488, 491, 511, 512, 522, 527, 530, 560, 585, 587]

    # print(len(hop_list),len(hop_list))

    # difference = list(set(other_list) - set(hop_list))
    # print(difference)